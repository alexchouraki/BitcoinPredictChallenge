---
title: "Bitcoin Prediction with NLP"
output:
  html_document: default
  html_notebook:
    number_sections: yes
    theme: flatly
---

```{r echo = FALSE}
library(tidyverse)
library(lubridate)
library(pracma)
library(ROCR)
library(randomForest)
```

# Notes générales
- Training dataset : prédiction du cours du bitcoin entre le 1er mai 2017 et le 1er mai 2018

- Test dataset : prédiction du cours du bitcoin entre le 1er mai 2018 et le 31 juillet 2018

# Chargement des données

## Bitcoin
```{r}
bitcoin.prices = read.csv('./data/btc_quote.csv') %>% select(-X)
bitcoin.prices$Date = ymd(bitcoin.prices$Date)
colnames(bitcoin.prices) = c('date', 'open', 'high', 'low', 'close', 'volume', 'market.cap', 'price')
```

Ajout du prix de cloture au jour J-1 et J-2:
```{r}
bitcoin.prices$close.D1 = c(NA, bitcoin.prices$close[c(1:(nrow(bitcoin.prices) - 1))])
bitcoin.prices$close.movavg = bitcoin.prices$close - movavg(bitcoin.prices$close, 7)
bitcoin.prices$close.D2 = c(NA, bitcoin.prices$close.D1[c(1:(nrow(bitcoin.prices) - 1))])
```

Ajout de la variable Up / Down :
```{r}
bitcoin.prices$trend = (bitcoin.prices$close - bitcoin.prices$close.D1) > 0
bitcoin.prices$trend = factor(bitcoin.prices$trend, levels = c(FALSE, TRUE), labels = c("Down", "Up"))

bitcoin.prices$trend.D1 = c(NA, bitcoin.prices$trend[c(1:(nrow(bitcoin.prices) - 1))])
bitcoin.prices$trend.D1 = factor(bitcoin.prices$trend.D1, levels = c(1, 2), labels = c("Up", "Down"))

bitcoin.prices$trend.D2 = c(NA, bitcoin.prices$trend.D1[c(1:(nrow(bitcoin.prices) - 1))])
bitcoin.prices$trend.D2 = factor(bitcoin.prices$trend.D2, levels = c(1, 2), labels = c("Up", "Down"))
```

```{r}
bitcoin.prices = bitcoin.prices %>% select(c(date, close.D1, close.D2, trend.D1, trend.D2, close, trend))
```

On fait enfin un filtre sur la date pour ne garder que ce qui nous intéresse.
```{r}
bitcoin.prices = bitcoin.prices %>% filter(date >= ymd('2017-05-01') & date <= ymd('2018-07-31'))
```

## Données de scraping
Ces données ont été générées à partir du dataset `NLP_Crypto_Dataset`, et traitées sur Jupyter (notebook `NLP_new_data`).
```{r}
NLP_data = read.csv('./data/NLP_grouped_data.csv')
```

On ajoute la différence entre le nombre de posts et sa moyenne glissante sur sept jours:
```{r}
NLP_data$post.movavg = NLP_data$post - movavg(NLP_data$post, 7)
```

On dédouble toutes les colonnes pour avoir leur valeur à J-1 :
```{r}
column_names = colnames(NLP_data)

for(column in colnames(NLP_data)){
  
  NLP_data = data.frame(NLP_data,
                        c(NA, NLP_data[[column]][c(1:(nrow(NLP_data)) - 1)]))
  
}

# Change column names to be a bit cleaner
colnames(NLP_data) = c(column_names, paste0(column_names, '.D1'))

# Remove date.D1 since it does not make any sense
NLP_data = NLP_data %>% dplyr::select(-date.D1)
```

De même, on fait un filtre sur les dates pour ne conserver que la période qui nous intéresse.
```{r}
NLP_data$date = ymd(NLP_data$date)
NLP_data = NLP_data %>% filter(date >= ymd('2017-05-01') & date <= ymd('2018-07-31'))
```

## Données Google Trends
Ces données ont été obtenues grâce au notebook Jupyter `GoogleTrendsGenerator`.

```{r}
GT_data = read.csv('./data/merged_trends.csv')
GT_data$date = ymd(GT_data$date)
colnames(GT_data) = c("date", paste0('gt.', colnames(GT_data)[2:ncol(GT_data)]))
GT_data
```

## Tout réunir ensemble
```{r}
full_data = inner_join(NLP_data, GT_data)
full_data = inner_join(full_data, bitcoin.prices)
full_data
```

On ajoute également le yield, qui est la variable qu'on va chercher à prédire avec certains modèles.
```{r}
full_data['yield'] <-( full_data['close'] - full_data['close.D1']) / full_data['close.D1']

full_data['yield.D1'] <-( full_data['close.D1'] - full_data['close.D2']) / full_data['close.D2']
```

On note également le yield moyen, afin de le réintégrer plus tard à nos prédictions.

```{r}
mean_yield = mean(full_data$yield)
```


## Enfin : définition des données train et test
```{r}
scale_data = full_data
scale_data[,-c(1,19,20,22)] = full_data[,-c(1,19,20,22)] %>% scale()

train_data = scale_data %>% filter(date >= ymd('2017/05/01') & date < ymd('2018/05/01'))
test_data = scale_data %>% filter(date >= ymd('2018/05/01') & date <= ymd('2018/07/31'))
```

# Premier modèle : régression logistique
## Avec toutes les variables
```{r}
full.reg.model = glm(as.formula(paste0("trend ~ ", paste0(colnames(full_data)[-c(21,22,23)], collapse = " + "))),
                family = "binomial",
                data = train_data)

summary(full.reg.model)
```

### ROC et AUC
```{r}
full.reg.predict = predict(full.reg.model, test_data, type = "response")
full.reg.prediction = prediction(full.reg.predict, test_data$trend)
full.reg.perf = performance(full.reg.prediction, measure = "tpr", x.measure = "fpr")
plot(full.reg.perf)
```

```{r}
reg.auc = performance(full.reg.prediction, measure = "auc")
reg.auc = reg.auc@y.values[[1]]
reg.auc
```

### Précision
```{r}
prediction_vector = rep(NA, nrow(test_data))
prediction_vector[predict(full.reg.model, test_data, type = "response") > 0.5] = "Up"
prediction_vector[is.na(prediction_vector)] = "Down"

print(paste0('Precision on test data: ', sum(prediction_vector == test_data$trend) / nrow(test_data)))
```

```{r}
prediction_vector = rep(NA, nrow(train_data))
prediction_vector[predict(full.reg.model, train_data, type = "response") > 0.5] = "Up"
prediction_vector[is.na(prediction_vector)] = "Down"

print(paste0('Precision on train data: ', sum(prediction_vector == train_data$trend) / nrow(train_data)))
```

On peut noter qu'il y a très peu de variables pour lesquelles la p-valeur est inférieur à 0.05 (seulement l'indice Google Trends pour le mot-clé "btc" et pour "hodl"). On va donc enlever quelques variables pour voir si cela peut aider à augmenter la précision.

## En enlevant quelques variables
```{r}
colnames(full_data)
```

```{r}
fewer.reg.model = glm(as.formula(paste0("trend ~ ", paste0(colnames(full_data)[-c(1,3,5,7,8,10,14,16,18,19,21,22,23)], collapse = " + "))),
                family = "binomial",
                data = train_data)

summary(fewer.reg.model)
```

### ROC et AUC
```{r}
fewer.reg.predict = predict(fewer.reg.model, test_data, type = "response")
fewer.reg.prediction = prediction(fewer.reg.predict, test_data$trend)
fewer.reg.perf = performance(fewer.reg.prediction, measure = "tpr", x.measure = "fpr")
plot(fewer.reg.perf)
```

```{r}
reg.auc = performance(fewer.reg.prediction, measure = "auc")
reg.auc = reg.auc@y.values[[1]]
reg.auc
```

### Précision
```{r}
prediction_vector = rep(NA, nrow(test_data))
prediction_vector[predict(fewer.reg.model, test_data, type = "response") > 0.5] = "Up"
prediction_vector[is.na(prediction_vector)] = "Down"

print(paste0('Precision on test data: ', sum(prediction_vector == test_data$trend) / nrow(test_data)))
```

```{r}
prediction_vector = rep(NA, nrow(train_data))
prediction_vector[predict(fewer.reg.model, train_data, type = "response") > 0.5] = "Up"
prediction_vector[is.na(prediction_vector)] = "Down"

print(paste0('Precision on train data: ', sum(prediction_vector == train_data$trend) / nrow(train_data)))
```

## En en enlevant encore plus

```{r}
colnames(full_data)
```

```{r}
few.reg.model = glm(as.formula(paste0("trend ~ ", paste0(colnames(full_data)[-c(1,3,5,6,7,8,10,11,14,16,18,19,21,22,23)], collapse = " + "))),
                family = "binomial",
                data = train_data)

summary(few.reg.model)
```

### ROC et AUC
```{r}
few.reg.predict = predict(few.reg.model, test_data, type = "response")
few.reg.prediction = prediction(few.reg.predict, test_data$trend)
few.reg.perf = performance(few.reg.prediction, measure = "tpr", x.measure = "fpr")
plot(few.reg.perf)
```

```{r}
reg.auc = performance(few.reg.prediction, measure = "auc")
reg.auc = reg.auc@y.values[[1]]
reg.auc
```

### Précision
```{r}
prediction_vector = rep(NA, nrow(test_data))
prediction_vector[predict(few.reg.model, test_data, type = "response") > 0.5] = "Up"
prediction_vector[is.na(prediction_vector)] = "Down"

print(paste0('Precision on test data: ', sum(prediction_vector == test_data$trend) / nrow(test_data)))
```

```{r}
prediction_vector = rep(NA, nrow(train_data))
prediction_vector[predict(few.reg.model, train_data, type = "response") > 0.5] = "Up"
prediction_vector[is.na(prediction_vector)] = "Down"

print(paste0('Precision on train data: ', sum(prediction_vector == train_data$trend) / nrow(train_data)))
```

Meilleure précision par cette méthode : $57.6\%$.

# Random forest
Malheureusement, les implémentations de Random Forest sur R ne permettent pas de fixer un max_depth. La seule chose à faire est donc de choisir les bonnes variables et d'optimiser le nombre de variables prises par l'arbre à chaque fois.

## Avec toutes les variables
```{r}
set.seed(0)
full.rf.model = randomForest(as.formula(paste0("trend ~ ", paste0(colnames(full_data)[-c(21,22,23)], collapse = " + "))),
                             ntree = 5000,
                             data = train_data)
```

```{r}
print(paste0("Precision on test data: ", sum(predict(full.rf.model, test_data, type = "response") == test_data$trend) / nrow(test_data)))
```

## Première sélection de variables
On garde la même sélection de variables que sur la deuxième sous-partie de la régression logistique.

```{r}
set.seed(0)
fewer.rf.model = randomForest(as.formula(paste0("trend ~ ", paste0(colnames(full_data)[-c(1,3,5,7,8,10,14,16,18,19,21,22,23)], collapse = " + "))),
                             ntree = 5000, mtry = 6, nodesize = 1,
                             data = train_data)
```

```{r}
print(paste0("Precision on test data: ", sum(predict(fewer.rf.model, test_data, type = "response") == test_data$trend) / nrow(test_data)))
```

Les résultats sont un peu meilleurs et dépassent le dummy classifier. Le paramètre mtry = 6 (6 variables essayées à chaque arbre) permet de maximiser la précision sur le test dataset.

## Deuxième sélection de variables (plus exigeante)
Ici, on prend la même sélection de variables que sur la troisième sous-partie de la régression logistique.

```{r}
set.seed(0)
few.rf.model = randomForest(as.formula(paste0("trend ~ ", paste0(colnames(full_data)[-c(1,3,5,6,7,8,10,11,14,16,18,19,21,22,23)], collapse = " + "))),
                             ntree = 5000, mtry = 4,
                             data = train_data)
```

```{r}
print(paste0("Precision on test data: ", sum(predict(few.rf.model, test_data, type = "response") == test_data$trend) / nrow(test_data)))
```

Les résultats sont sensiblement moins bons que dans le cas précédent.

## A présent, essayons plusieurs modèles afin de prédire au mieux le yield.
Le signe du yield nous indiquera le trend.

L'objectif de cette partie est de tester, successivement : une régression linéaire classique, des random forests, un XGBoost, et un modèle GARCH, puis de les combiner de façon appropriée afin d'avoir la meilleure estimation possible du yield, qui devrait nous donner une bonne estimation du trend.

### Régression linéaire
```{r}
full.reglin = lm(as.formula(paste0("yield ~ ", paste0(colnames(full_data)[-c(21,22,23)], collapse = " + "))),,
                data = train_data)

summary(full.reglin)
```
 
```{r}
prediction_vector = rep(NA, nrow(train_data))
prediction_vector <- predict(full.reglin, train_data, type = "response")
mean((prediction_vector-train_data$yield)**2)

```
Evidemment, avec toutes les variables, ce n'est pas terrible.
Utilisons leaps pour récupérer un subset de variables plus approprié.

```{r}
library(leaps)
```

```{r}
train_data[,-c(3,7, 5,11, 19, 20, 21,22,23)]
```
On enlève les variables les moins significatives du modèle global, sinon leaps mettra trop de temps à tester les combinaisons.

```{r}
#leaps(train_data[,-c(3,7, 5,11, 19, 20,21,22,23)],train_data[,23], int = TRUE, method = "adjr2", nbest = 5)
```

Ca prend quand même trop de temps... Du coup, faisons un PCA afin de voir quelles variables synthétisent le plus d'information.

```{r}
library(FactoMineR)
```

```{r}
res.PCA <- PCA(train_data[,-c(1,3,7, 5,11, 19, 20,21,22,23)])
```

```{r}
summary(res.PCA)
```

Gardons ces dix variables pour l'instant, puis nous verrons si nous pouvons améliorer le modèle.

```{r}
reglin2 = lm(yield ~ polarity + post + post.movavg+ log_merit.D1+ post.D1+ polarity_and_log_merit.D1+ gt.bitcoin+ gt.btc + gt.cryptocurrency+ gt.hodl,
                data = train_data)

summary(reglin2)
```

```{r}
reglin3 = lm(yield ~ post + gt.bitcoin+ gt.btc + gt.cryptocurrency+ gt.hodl,
                data = train_data)

summary(reglin3)
```

Maintenant, testons ces 3 modèles, afin de voir lequel nous permet d'obtenir les meilleures prédictions en terme de trend.

```{r}
pred <- test_data[,c(1,23,22)]
pred['pred_yield_lm1'] <- predict(full.reglin, test_data, type = "response")
pred['pred_yield_lm2'] <- predict(reglin2, test_data, type = "response")
pred['pred_yield_lm3'] <- predict(reglin3, test_data, type = "response")
```

```{r}
pred['pred_trend_lm1'] <- sign(pred['pred_yield_lm1']+mean_yield)
pred['pred_trend_lm2'] <- sign(pred['pred_yield_lm2']+mean_yield)
pred['pred_trend_lm3'] <- sign(pred['pred_yield_lm3']+mean_yield)
```

```{r}
pred['trend_int'] <- (pred['trend']=='Down')*-1 + (pred['trend']=='Up')*1 
```

Voyons la performance de ces modèles sur le trend: 
```{r}
sum(pred['pred_trend_lm1']==pred['trend_int'])/length(test_data[,1])
sum(pred['pred_trend_lm2']==pred['trend_int'])/length(test_data[,1])
sum(pred['pred_trend_lm3']==pred['trend_int'])/length(test_data[,1])
```
On obtient exactement les mêmes réponses pour lm1 et lm2 ; le 3e modèle est renvoie des résultats différents, mais la performance finale est la même...
Ce n'est pas terrible, la régression logistique paraissait marcher nettement mieux...

### XGboost

```{r}
library(xgboost)
```

L'installation du package échoue...

### Garch

```{r}
library(tseries)
```

L'installation du package échoue aussi...

```{r}
library(fGarch)
```
```{r}
gfit <- garchFit(formula = ~ garch(2, 2), data = train_data[,"yield"], cond.dist="norm")
```

```{r}
#pred['pred_yield_garch'] <- predict(gfit, test_data[,'yield'], type = "response")
```


